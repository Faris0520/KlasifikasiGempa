# -*- coding: utf-8 -*-
"""Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MJ3Jwjg2hL2eTO04NsnVrGaLsHVcNDfL

# **Langkah 1. Pemuatan dan Inspeksi Data**

Mengimpor dataset dan melihat 5 data awal
"""

import pandas as pd
import numpy as np

data = "https://raw.githubusercontent.com/Faris0520/filecolab/refs/heads/main/Data%20Mining/katalog_gempa.csv"
df = pd.read_csv(data)
df.head()

"""Melihat informasi dataset"""

df.info()

"""# **Langkah 2. Menyortir Data**

Kode ini untuk milih kolom,
lalu di sort menurut datetime, lalu di sort lagi hanya sampai 30000 baris, dan menghasilkan file hasil_sort.csv
"""

kolom_sort = df.sort_values(by='datetime', ascending=False)
kolom = kolom_sort.iloc[2600:32600, 0:10]

kolom.to_csv('hasil_sort.csv', index=False)

"""# **Langkah 3. Menangani Missing Value**

Impor lagi data bersih tadi.
"""

import pandas as pd
import numpy as np
databersih = 'https://raw.githubusercontent.com/Faris0520/filecolab/refs/heads/main/Data%20Mining/hasil_sort.csv'
df = pd.read_csv(databersih)
df.info()

"""Menampilkan 5 data awal"""

df.head()

"""Mencari missing value"""

print("Jumlah missing value:")
print(df.isnull().sum())

"""Isi missing value"""

df_filled = df.copy()

# Kategorikal
for col in df_filled.select_dtypes(include='object').columns:
  df_filled[col] = df_filled[col].fillna(df_filled[col].mode()[0])

# Numerikal
for col in df_filled.select_dtypes(include=[np.number]).columns:
  df_filled[col] = df_filled[col].fillna(df_filled[col].median())

print("\nJumlah Missing Value per kolom:")
print(df_filled.isnull().sum())

df = df_filled

"""Cari Outlier"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.boxplot(x = df["phasecount"])

sns.boxplot(x = df["depth"])

"""Menghapus Outlier"""

kolomNumerik = ["phasecount", "depth"]

df_clean = df.dropna()

for col in kolomNumerik:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df[f'outlier_{col}'] = df[col].apply(
        lambda x: 'Outlier' if (x < lower_bound or x > upper_bound) else 'Normal'
    )

    print(f"\nAnalisis kolom: {col}")
    print(f"Q1 (25%)       : {Q1}")
    print(f"Q3 (75%)       : {Q3}")
    print(f"IQR            : {IQR}")
    print(f"Lower Bound    : {lower_bound}")
    print(f"Upper Bound    : {upper_bound}")
    print(f"\n{col} → Lower: {lower_bound:.2f}, Upper: {upper_bound:.2f}")
    print(df[f'outlier_{col}'].value_counts())

# Cek hasil
print("Jumlah data setelah filter:", len(df))
print("Magnitudo minimum sekarang:", df['magnitude'].min())
print("Magnitudo maksimum sekarang:", df['magnitude'].max())

df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
print("Data setelah outlier dihapus:", df.shape)

df.to_csv("gempa_clean.csv", index=False)

sns.boxplot(x = df["phasecount"])

"""Visualisasi Hasil Sementara"""

plt.figure(figsize=(10,5))
sns.histplot(df_clean['magnitude'], bins=30, kde=True)
plt.title('Distribusi Magnitudo Gempa di Indonesia (2023–2025)')
plt.xlabel('Magnitudo')
plt.ylabel('Frekuensi')
plt.show()

plt.figure(figsize=(10,5))
sns.scatterplot(x='longitude', y='latitude', hue='magnitude', data=df_clean, palette='grey', s=10)
plt.title('Sebaran Lokasi Gempa di Indonesia')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

"""# **Langkah 4. Klasifikasi Magnitudo**

Klasifikasi kelas magnitudo
"""

df = pd.read_csv("gempa_clean.csv")
def kategori_magnitude(m):
    if m < 3.0:
        return 'Ringan'
    elif 3.0 <= m < 5.0:
        return 'Sedang'
    else:
        return 'Kuat'

df['kategori_magnitude'] = df['magnitude'].apply(kategori_magnitude)
df.head()

"""Normalisasi Kategorikal menggunakan LabelEncoder"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['mag_type_encoded'] = le.fit_transform(df['mag_type'])
df

"""Normalisasi numerik dengan Min-Max"""

from sklearn.preprocessing import MinMaxScaler

features = ['latitude', 'longitude', 'depth', 'phasecount', 'azimuth_gap', 'mag_type_encoded']
X = df[features]
y = df['kategori_magnitude']

# Normalisasi fitur numerik
scaler = MinMaxScaler()
X[['latitude', 'longitude', 'depth', 'phasecount', 'azimuth_gap']] = scaler.fit_transform(
    X[['latitude', 'longitude', 'depth', 'phasecount', 'azimuth_gap']])

"""# **Langkah 5. Training model**"""

# Import library tambahan
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load data hasil cleaning (dari langkah sebelumnya)
df = pd.read_csv("gempa_clean.csv")

# Encode fitur kategorikal
le_magtype = LabelEncoder()
df['mag_type_encoded'] = le_magtype.fit_transform(df['mag_type'])
df['kategori_magnitude'] = df['magnitude'].apply(kategori_magnitude)

le_location = LabelEncoder()
df['location_encoded'] = le_location.fit_transform(df['location'])

# Pilih fitur dan target
features = ['latitude', 'longitude', 'depth', 'phasecount', 'azimuth_gap', 'mag_type_encoded']
X = df[features]
y = df['kategori_magnitude']

# Normalisasi fitur numerik
scaler = MinMaxScaler()
X[['latitude', 'longitude', 'depth', 'phasecount', 'azimuth_gap']] = scaler.fit_transform(
    X[['latitude', 'longitude', 'depth', 'phasecount', 'azimuth_gap']])

# Split data latih dan uji
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42)

# Bangun model Naïve Bayes
model = GaussianNB()
model.fit(X_train, y_train)

# Prediksi
y_pred = model.predict(X_test)

# Evaluasi
acc = accuracy_score(y_test, y_pred)

"""Hasil Klasifikasi"""

print(f"Akurasi: {acc:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""Grafik Confusion Matrix"""

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.title("Confusion Matrix Naïve Bayes")
plt.show()

df.to_csv("gempa_final.csv", index=False)